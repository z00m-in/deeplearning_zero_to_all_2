{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoIxBstW7GFl9XFcqvY3oB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z00m-in/deeplearning_zero_to_all_2/blob/main/%EB%AA%A8%EB%91%90%EB%94%A52_Lab07_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja5AKb6PjZXk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 데이터 세팅"
      ],
      "metadata": {
        "id": "bhXoAbsRzL4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
        "\n",
        "x_train = xy[:, 0:-1]\n",
        "y_train = xy[:, [-1]]"
      ],
      "metadata": {
        "id": "oxDQPNp3zMwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
        "W = tf.Variable(tf.random_normal([4, 1]), dtype=tf.float32)\n",
        "b = tf.Variable(tf.random_normal([1]), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "mUv613-yz0vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화: Normalization"
      ],
      "metadata": {
        "id": "XyKb9mvN0o8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규화 실행\n",
        "def normalization(data):\n",
        "  numerator = data - np.min(data, 0)\n",
        "  denominator = np.max(data, 0) - np.min(data, 0)\n",
        "  return numerator / denominator\n",
        "\n",
        "xy = normalization(xy)"
      ],
      "metadata": {
        "id": "NMgPXgcVzaV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2 Normalization"
      ],
      "metadata": {
        "id": "XVGp5ddC14ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearReg_fn(features):\n",
        "  hypothesis = tf.matmul(features, W) + b\n",
        "  return hypothesis"
      ],
      "metadata": {
        "id": "ip2KNvpDzzSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존의 cost function에 λ라는 특정 값을 추가해 regularization이 실현되는 경우.\n",
        "# 함수의 beta가 λ에 해당\n",
        "def l2_loss(loss, beta = 0.01):\n",
        "  W_reg = tf.nn.l2_loss(W) # output = sum(t ** 2) / 2\n",
        "  loss = tf.reduce_mean(loss + W_reg * beta)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "3E4WaawP1Akx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(hypothesis, labels, flag = False):\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - labels))\n",
        "  if(flag):\n",
        "    cost = l2_loss(cost)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "TgLcNHVs1Bwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Decay"
      ],
      "metadata": {
        "id": "o3lm97hk17kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_decay = True\n",
        "starter_learning_rate = 0.1\n",
        "\n",
        "if(is_decay):\n",
        "  global_step = tf.Variable(0, trainable=False)\n",
        "  # exponential_decay(시작값, 전체 step, 몇 번의 step 마다, 얼만큼의 값을 조절할지)\n",
        "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 50, 0.96, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "else:\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=starter_learning_rate)"
      ],
      "metadata": {
        "id": "K2oe3SdB0AIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(features, labels, l2_flag):\n",
        " with tf.GradientTape() as tape:\n",
        "   # 가설의 결과와 실제 값을 비교하면서 loss 값 구하고 L2 flag 조정\n",
        "   loss_value = loss_fn(linearReg_fn(features),labels, l2_flag)\n",
        " return tape.gradient(loss_value, [W,b]), loss_value"
      ],
      "metadata": {
        "id": "c8FQrvyJ0Fmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 실행 부분\n",
        "for step in range(EPOCHS):\n",
        "  for features, labels in tfe.Iterator(dataset):\n",
        "    features = tf.cast(features, tf.float32)\n",
        "    labels = tf.cast(labels, tf.float32)\n",
        "    grads, loss_value = grad(linearReg_fn(features), features, labels, False)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]), global_step=global_step)\n",
        "    if step % 10 == 0:\n",
        "      print(\"Iter: {}, Loss: {:.4f}, Learning Rate: {:.8f}\".format(step, loss_value, optimizer._learning_rate()))"
      ],
      "metadata": {
        "id": "Gks8lG_n0G_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}